import os
from langchain.schema import Document
from langchain_core.runnables import RunnableConfig
from langchain_community.document_loaders import TextLoader
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langgraph.graph.state import StateGraph, CompiledStateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered
from graph.graph_state import GraphState
from chains.summary import SummaryChain
from chains.generate import GenerateChain

def route_question(state: GraphState) -> str:
    """
    æ ¹æ®æ“ä½œç±»å‹è·¯ç”±åˆ°ç›¸åº”çš„å¤„ç†èŠ‚ç‚¹ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€

    è¿”å›:
        str: ä¸‹ä¸€ä¸ªè¦è°ƒç”¨çš„èŠ‚ç‚¹åç§°
    """
    print("--- ğŸ¤– æ­£åœ¨æ ¹æ®ç±»å‹é€‰æ‹©åˆ†æ”¯ ---")
    if state['type'] == 'websearch':
        return "extract_keywords"
    if state['type'] == 'file':
        return "file_process"
    elif state['type'] == 'chat':
        return "generate"

def generate(state: GraphState) -> GraphState:
    """
    æ ¹æ®æ–‡æ¡£å’Œå¯¹è¯å†å²ç”Ÿæˆç­”æ¡ˆã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€

    è¿”å›:
        state (GraphState): è¿”å›æ·»åŠ äº†LLMç”Ÿæˆå†…å®¹çš„æ–°çŠ¶æ€
    """
    print("--- ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­” ---")
    chain = GenerateChain(state["model_name"], state["temperature"])
    messages = state["messages"]
    state["messages"] = chain.invoke({
        "question": messages[-1].content,
        "history": messages[:-1],
        "documents": state["documents"]
    })
    return state

def file_process(state: GraphState, config: RunnableConfig) -> GraphState:
    """
    å¤„ç†æ–‡ä»¶

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€
        config (RunnableConfig): å¯è¿è¡Œé…ç½®

    è¿”å›:
        state (GraphState): è¿”å›å›¾çŠ¶æ€ï¼Œå°†æ–‡æ¡£æ·»åŠ  config ä¸­çš„å‘é‡å­˜å‚¨
    """

    print("--- ğŸ¤– å¼€å§‹å¤„ç†æ–‡ä»¶ ---")
    vector_store = config["configurable"]["vectorstore"]

    for doc in state["documents"]:
        file_path: str = doc.page_content
        if os.path.exists(file_path):
            print(f"--- ğŸ“„ æ–‡ä»¶è·¯å¾„: {file_path}")
            split_docs: list[Document] = None
            if file_path.endswith(".txt") or file_path.endswith(".md"):
                # å¤„ç†æ–‡æœ¬æˆ–Markdownæ–‡ä»¶
                docs = TextLoader(file_path, autodetect_encoding=True).load()
                # æ–‡æœ¬åˆ†å‰²
                splitter = RecursiveCharacterTextSplitter(
                    separators=["\n\n", "\n", " ", ".", ",", "\u200B", "\uff0c", "\u3001", "\uff0e", "\u3002", ""],
                    chunk_size=512,
                    chunk_overlap=256,
                    add_start_index=True
                )
                split_docs = splitter.split_documents(docs)
            else:
                # ä½¿ç”¨ marker-pdf å¤„ç†å…¶ä»–æ–‡ä»¶
                converter = PdfConverter(artifact_dict=create_model_dict())
                rendered = converter(file_path)
                docs, _, _ = text_from_rendered(rendered)
                splitter = MarkdownHeaderTextSplitter(
                    [("#", "Header 1"), ("##", "Header 2"), ("###", "Header 3")],
                    strip_headers = False
                )
                split_docs = splitter.split_text(docs)

            # å°†å¤„ç†åçš„æ–‡æ¡£æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä¸­
            vector_store.add_documents(split_docs)
        else:
            print(f"--- ğŸ“„ æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨: {file_path}")
    return state

def extract_keywords(state: GraphState, config: RunnableConfig) -> GraphState:
    """
    ä»é—®é¢˜ä¸­æå–å…³é”®è¯ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€
        config (RunnableConfig): å¯è¿è¡Œé…ç½®

    è¿”å›:
        state (GraphState): è¿”å›æ·»åŠ äº†æå–å…³é”®è¯çš„æ–°çŠ¶æ€
    """

    print("--- ğŸ¤– æ­£åœ¨æå–å…³é”®è¯ ---")
    chain = SummaryChain(state["model_name"], state["temperature"])
    messages = state["messages"]
    query = chain.invoke({"question": messages[-1].content, "history": messages[:-1]})

    if state["type"] == "websearch":
        # å°†ç”Ÿæˆçš„æœç´¢æŸ¥è¯¢æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­ï¼Œä¸‹ä¸€ä¸ªèŠ‚ç‚¹å°†ä¼šä½¿ç”¨
        state["messages"] = query
    elif state["type"] == "file":
        # ä½¿ç”¨ç”Ÿæˆçš„æœç´¢æŸ¥è¯¢åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢
        # docs = config["configurable"]["vectorstore"].max_marginal_relevance_search(query.content, 5)
        docs = config["configurable"]["vectorstore"].similarity_search(query.content, 5)
        print(f"--- ğŸ“„ å¬å›ç»“æœ:")
        for idx, doc in enumerate(docs):
            print(f"============================ doc-{idx + 1}  {doc.metadata} ============================ ")
            print(doc.page_content)
        state["documents"] = docs
    return state

def decide_to_generate(state: GraphState) -> str:
    """
    å†³å®šæ˜¯è¿›è¡Œç½‘ç»œæœç´¢è¿˜æ˜¯ç›´æ¥ç”Ÿæˆå›ç­”ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€

    è¿”å›:
        str: ä¸‹ä¸€ä¸ªè¦è°ƒç”¨çš„èŠ‚ç‚¹åç§°
    """

    if state["type"] == "websearch":
        print("--- ğŸŒ éœ€è¦è¿›è¡Œç½‘ç»œæœç´¢ ---")
        return "websearch"
    elif state["type"] == "file":
        print("--- â­ æ— éœ€æœç´¢ï¼Œç›´æ¥ç”Ÿæˆç­”æ¡ˆ ---")
        return "generate"

def web_search(state: GraphState) -> GraphState:
    """
    åŸºäºé—®é¢˜è¿›è¡Œç½‘ç»œæœç´¢ã€‚

    å‚æ•°:
        state (GraphState): å½“å‰å›¾çš„çŠ¶æ€

    è¿”å›:
        state (GraphState): è¿”å›æ·»åŠ äº†ç½‘ç»œæœç´¢ç»“æœçš„æ–°çŠ¶æ€
    """

    print("---ğŸŒ æ­£åœ¨è¿›è¡Œç½‘ç»œæœç´¢ ---")
    web_search_tool = TavilySearchResults(k = 3)
    documents = state["documents"]
    try:
        docs = web_search_tool.invoke({"query": state["messages"][-1].content})
        web_results = "\n".join([d["content"] for d in docs])
        web_results = Document(page_content=web_results)
        documents.append(web_results)
        state["documents"] = documents
    except:
        pass
    print(f"ğŸŒ æœç´¢ç»“æœ:\n{documents}")
    return state

def create_graph() -> CompiledStateGraph:
    """
    åˆ›å»ºå¹¶é…ç½®çŠ¶æ€å›¾å·¥ä½œæµã€‚

    è¿”å›:
        CompiledStateGraph: ç¼–è¯‘å¥½çš„çŠ¶æ€å›¾
    """

    workflow = StateGraph(GraphState)
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("websearch", web_search)
    workflow.add_node("extract_keywords", extract_keywords)
    workflow.add_node("file_process", file_process)
    workflow.add_node("generate", generate)
    # æ·»åŠ è¾¹
    workflow.set_conditional_entry_point(
        route_question,
        {
            "extract_keywords": "extract_keywords",
            "generate": "generate",
            "file_process": "file_process",
        },
    )
    workflow.add_edge("file_process", "extract_keywords")
    workflow.add_conditional_edges(
        "extract_keywords",
        decide_to_generate,
        {
            "websearch": "websearch",
            "generate": "generate",
        },
    )
    workflow.add_edge("websearch", "generate")
    workflow.add_edge("generate", END)

    # åˆ›å»ºå›¾ï¼Œå¹¶ä½¿ç”¨ `MemorySaver()` åœ¨å†…å­˜ä¸­ä¿å­˜çŠ¶æ€
    return workflow.compile(checkpointer=MemorySaver())

def stream_graph_updates(graph: CompiledStateGraph, user_input: GraphState, config: dict):
    """
    æµå¼å¤„ç†å›¾æ›´æ–°å¹¶è¿”å›æœ€ç»ˆç»“æœã€‚

    å‚æ•°:
        graph (CompiledStateGraph): ç¼–è¯‘å¥½çš„çŠ¶æ€å›¾
        user_input (GraphState): ç”¨æˆ·è¾“å…¥çš„çŠ¶æ€
        config (dict): é…ç½®å­—å…¸

    è¿”å›:
        generator: ç”Ÿæˆå™¨å¯¹è±¡ï¼Œé€æ­¥è¿”å›å›¾æ›´æ–°çš„å†…å®¹
    """

    for chunk, _ in graph.stream(user_input, config, stream_mode="messages"):
        yield chunk.content
