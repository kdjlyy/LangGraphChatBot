import os
import torch
torch.classes.__path__ = [os.path.join(torch.__path__[0], torch.classes.__file__)]
import uuid
from langchain.schema import Document
from streamlit_extras.bottom_container import bottom
from chains.models import load_vector_store, load_rerank
from graph.graph import create_graph, stream_graph_updates
from utils.common import *

# åŠ è½½ .env åˆ°ç¯å¢ƒå˜é‡
load_dotenv(verbose=True)

# è®¾ç½®é¡µé¢é…ç½®ä¿¡æ¯
st.set_page_config(
    page_title="AI èŠå¤©æœºå™¨äºº",
    page_icon="ğŸ¤–",
    layout="wide"
)

# Fixme: å‡å°‘åŠ è½½é¢‘ç‡
env = load_env_vars()
# å®šä¹‰å¯¹è¯ç±»å‹é€‰é¡¹
type_options = {"â­ï¸ ç¦»çº¿å¯¹è¯": "chat", "ğŸŒ è”ç½‘æœç´¢": "websearch", "âŒ¨ï¸ ä»£ç æ¨¡å¼": "code"}

# åˆå§‹åŒ–ä¸Šä¼ çŠ¶æ€ã€æ¨¡å‹åç§°å’Œå¯¹è¯ç±»å‹
if "settings" not in st.session_state:
    # é»˜è®¤ä¸ºç¦»çº¿å¯¹è¯
    st.session_state.settings = {"type": "chat", "uploaded": False, "search_num": env["SEARCH_NUN"]}
    st.session_state.embedding_selectbox_disable = False
# åˆå§‹åŒ–ä¼šè¯IDå’Œå‘é‡å­˜å‚¨
if "config" not in st.session_state:
    st.session_state.config = {"configurable": {"thread_id": uuid.uuid4().hex, "vectorstore": None, "rerank": None}}
# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€å˜é‡ï¼Œåˆ›å»ºå›¾
if "graph" not in st.session_state:
    st.session_state.graph = create_graph()
# åˆå§‹åŒ–å¯¹è¯å†å²è®°å½•
if "history" not in st.session_state:
    st.session_state.history = []

# æ˜¾ç¤ºåº”ç”¨æ ‡é¢˜
st.subheader("æˆ‘å¯ä»¥å¸®ä½ :blue[å†™ä»£ç ã€è¯»æ–‡ä»¶ã€è”ç½‘æœç´¢è§£å†³å„ç§é—®é¢˜]ï¼Œæ¬¢è¿å‘æˆ‘æé—®ï½ ğŸ˜¸", divider="gray")

# å®šä¹‰å¯é€‰çš„æ¨¡å‹
model_options = {"é€šä¹‰åƒé—®": "Qwen/QwQ-32B", "DeepSeek R1": "deepseek-ai/DeepSeek-R1"}

# é‡æ–°åˆ›å»ºå›¾
def rebuild_graph():
    st.session_state.graph = create_graph()

# ä¾§è¾¹æ è®¾ç½®éƒ¨åˆ†
with st.sidebar:
    st.header("è®¾ç½®")
    st.divider()

    # æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
    env['CURRENT_MODEL'] = st.selectbox(
        label="é€‰æ‹©æ¨¡å‹",
        options=env["AVAILABLE_MODEL_LIST"],
        index=0,
        help="é€‰æ‹© LLM æ¨¡å‹çš„ç§ç±»",
        on_change=rebuild_graph
    )
    # æ¨¡å‹æ¸©åº¦æ»‘åŠ¨æ¡
    env['TEMPERATURE'] = st.slider(
        label="æ¨¡å‹æ¸©åº¦",
        min_value=0.0, max_value=1.0, value=0.0,
        help="æ¨¡å‹æ¸©åº¦ï¼ˆTemperatureï¼‰å‚æ•°ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºçš„å¤šæ ·æ€§å’Œç¡®å®šæ€§ã€‚é«˜ Temperature å¢åŠ å¤šæ ·æ€§ä½†å¯èƒ½é™ä½ç¡®å®šæ€§ï¼Œä½ Temperature åˆ™å¢åŠ ç¡®å®šæ€§ä½†å¯èƒ½é™ä½å¤šæ ·æ€§ã€‚"
    )
    st.divider()
    st.selectbox(
        key="embedding_model_selectbox",
        label="é€‰æ‹©åµŒå…¥æ¨¡å‹",
        options=env["AVAILABLE_EMBEDDING_MODEL_LIST"],
        index=2,
        help="é€‰æ‹©åµŒå…¥æ¨¡å‹çš„ç§ç±»",
    )

    st.session_state.settings["model_name"] = env['CURRENT_MODEL']
    st.session_state.settings["temperature"] = env['TEMPERATURE']

    if not st.session_state.config["configurable"]["vectorstore"]:
        st.session_state.config["configurable"]["vectorstore"] = load_vector_store(st.session_state.embedding_model_selectbox)

    if not st.session_state.config["configurable"]["rerank"]:
        st.session_state.config["configurable"]["rerank"] = load_rerank()
    st.divider()

    # è‡ªå®šä¹‰é“¾æ¥
    st.caption(f"{datetime.datetime.now().strftime('%Y.%m')} - [LangGraphChatBot](https://github.com/kdjlyy/LangGraphChatBot)")

question = None

with bottom():
    # åº•éƒ¨å®¹å™¨ï¼ŒåŒ…å«å·¥å…·é€‰æ‹©ã€æ–‡ä»¶ä¸Šä¼ å’Œè¾“å…¥æ¡†
    st.session_state.settings["type"] = type_options[st.radio("å·¥å…·é€‰æ‹©", type_options.keys(), horizontal=True, label_visibility="collapsed", index=list(type_options.values()).index(st.session_state.settings["type"]))]
    # æ–‡ä»¶ä¸Šä¼ ç»„ä»¶, pdfã€docã€xlsx æ ¼å¼çš„æ–‡ä»¶å¯èƒ½é€ æˆç³»ç»Ÿèµ„æºä¸è¶³
    # uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=["txt", "md", "pdf", "doc", "xls", "xlsx"], accept_multiple_files=False, label_visibility="collapsed")
    uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶", type=["txt", "md"], accept_multiple_files=False, label_visibility="collapsed")
    # èŠå¤©è¾“å…¥æ¡†
    question = st.chat_input('è¾“å…¥æ‚¨è¦è¯¢é—®çš„å†…å®¹ï¼Œshift + enter æ¢è¡Œ')

# æ˜¾ç¤ºå†å²å¯¹è¯å†…å®¹
for message in st.session_state.history:
    with st.chat_message(message["role"]):
      st.markdown(message["content"])

# å¤„ç†ç”¨æˆ·æé—®
if question:
    # æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
    with st.chat_message("user"):
        st.markdown(question)

    # å‡†å¤‡è¯·æ±‚çŠ¶æ€
    state = []
    message = [{"role": "system", "content": f"å½“å‰æ—¥æœŸæ˜¯ï¼š{get_current_time()}"}, {"role": "user", "content": question}]
    if st.session_state.settings["type"] == "code":
        # ä»£ç æ¨¡å¼ä½¿ç”¨ä¸“é—¨çš„ä»£ç æ¨¡å‹
        state = {"model_name": env["CODE_MODEL"], "temperature": st.session_state.settings["temperature"],
                 "messages": message, "type": "chat", "documents": [],  "search_num": env["SEARCH_NUN"]}
    else:
        # å…¶ä»–æ¨¡å¼ä½¿ç”¨é€‰æ‹©çš„æ¨¡å‹
        state = {"model_name": st.session_state.settings["model_name"], "temperature": st.session_state.settings["temperature"],
                 "messages": message, "type": st.session_state.settings["type"], "documents": [], "search_num": env["SEARCH_NUN"]}

    # å¤„ç†æ–‡ä»¶ä¸Šä¼ 
    if uploaded_file:
        state["type"] = "file"
        if not st.session_state.settings["uploaded"]:
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
            file_path = upload_pdf(uploaded_file)
            # æ·»åŠ æ–‡æ¡£åˆ°è¯·æ±‚
            state["documents"].append(Document(page_content=file_path))
            st.session_state.settings["uploaded"] = True
        else:
            st.error("è¯·åˆ·æ–°é¡µé¢åå†ä¸Šä¼ æ–‡ä»¶")

    # è·å–AIå›ç­”å¹¶ä»¥æµå¼æ–¹å¼æ˜¾ç¤º
    answer = st.chat_message("assistant").write_stream(stream_graph_updates(st.session_state.graph, state, st.session_state.config))

    # å°†å¯¹è¯ä¿å­˜åˆ°å†å²è®°å½•
    st.session_state.history.append({"role": "user", "content": question})
    st.session_state.history.append({"role": "assistant", "content": answer})
